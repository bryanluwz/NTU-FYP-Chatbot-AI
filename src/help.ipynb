{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded vector store from local storage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "📝 Query Result:\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Page 1  SC1015 : Course  Information  \n",
      "Introduction  to Data Science  and Artificial  Intelligence  \n",
      " \n",
      " \n",
      " \n",
      "Course  Instructors Dr K G  Smitha (Co-Ord) smitha@ntu.edu.sg  Office  : N4-02c-75 \n",
      "LAMS  and Reviews  Mr. Ong Chin Ann chinann.ong@ntu.edu.sg  Office  : N4-02c-108 \n",
      " \n",
      "Lab Instructors   \n",
      "Graduate  Teaching  Assistants   \n",
      "Note email  of your Lab’s TA   \n",
      "Lab Exercises  (will work closely  with Co-Ord) Details  posted  in TimeTable   \n",
      " \n",
      " \n",
      "General Information  \n",
      "Description  \n",
      "In today's  era of Information,  ‘Data’  is the new driving force, provided we \n",
      "know  how to extract  relevant  ‘Intelligence’.  \n",
      "This course  will start with the core principles  of Data Science,  and will  equip  \n",
      "you with the basic tools and techniques  of data handling,  exploratory  data \n",
      "analysis,  data visualization,  data-based  inference,  and data-focused  \n",
      "communication.  The course  will also introduce  you to the fundamentals  of\n",
      "\n",
      "communicate  the problem  and the inference,  outline  the roles and requirements  of artificial  intelligence  in practical  \n",
      "applications,  apply  basic artificial  intelligence  techniques  in search  problems  and game  playing,  and discuss  and explain  \n",
      "concepts  of computer  vision  and natural  language  processing.  \n",
      " \n",
      " \n",
      "Course  Material  \n",
      "There  is no single  textbook for  the course.  The following books  and resources  will be used as  references  time and again.  \n",
      "1. Python  Data Science  Handbook  : Jake VanderPlas : O’Reilly  (1st edition)  \n",
      "2. An Introduction  to Statistical  Learning  : James,  Witten, Hastie,  Tibshirani  \n",
      "3. Artificial  Intelligence:  A Modern  Approach : Russell  and Norvig  (3rd edition)  \n",
      "Additional  resources,  if required,  will be shared  with you in the LAMS  videos,  face-to-face lectures,  and example  classes.\n",
      "\n",
      "Artificial  Intelligence  – state space,  search  problems,  computer  vision  and \n",
      "natural  language  processing.  \n",
      "The course  will motivate  you to work closely  with data and make  data-driven  \n",
      "decisions  in your field of study.  The course  will also touch  upon ethical  issues \n",
      "in Data Science  and Artificial  Intelligence,  and motivate  you to explore  the \n",
      "cutting -edge applications  related  to Big Data,  Neural  Networks  and Deep \n",
      "Learning.  Python  will be the language  of choice  to introduce  hands -on \n",
      "computational  techniques.  \n",
      " \n",
      "Outcome  \n",
      "By the end of this course,  you should  be able to identify  and define  data- \n",
      "oriented  problems  and data-driven  decisions  in real life, discuss and illustrate  \n",
      "the problems  in terms  of data exploration  and visualization,  apply  basic \n",
      "machine  learning  tools to extract  inferential  information  from the data,  compose  an engaging  “data- story”  to\n",
      "\n",
      "Module  06 1 Week  Artificial  Intelligence  – Current  State -of-the-Art No Lab Session  for this Module  \n",
      "Module  07 2 Weeks  Intelligent  Agents  and Search  Space  Solutions  Uninformed  and Informed  Search  \n",
      "Module  08 1 Week  Constraint  Satisfaction  and Game  Playing  Game  with Constrained  Search  \n",
      "Module  09 e-Learning  Miscellaneous  topics  in Artificial  Intelligence  No Lab Session  for this Module  \n",
      " \n",
      "Miscellaneous  topics in  AI may include  basic introduction  to cutting -edge practical  domains  like Computer  Vision,  \n",
      "Natural  Language  Processing, Reinforcement  Learning,  as well as Ethics in  the context  of Data Science and AI.\n",
      "\n",
      "Question: What is the purpose of SC1015?\n",
      "Helpful Answer:, \n",
      "3. An Introduction  to Statistical  Learning  : David,        Introduction  to Statistical  Learning  : David,      \n",
      "4. The Introduction  to Statistical  Learning  : Kevin,    \n",
      "5. The Introduction  to Statistical  Learning  : David,    \n",
      "6. The Introduction  to Statistical  Learning  : David,    \n",
      "7. The Introduction  to Statistical  Learning  : David,      \n",
      "8. The Introduction  to Statistical  Learning  : David,      \n",
      "9. The Introduction  to Statistical  Learning  : David,      \n",
      "10. The Introduction  to Statistical  Learning  : David,    \n",
      "11. The Introduction  to Statistical  Learning  : David,     \n",
      "12. The Introduction  to Statistical  Learning  : David,      \n",
      "13. The Introduction  to Statistical  Learning  : David,    \n",
      "14. The Introduction  to Statistical  Learning  : David,    \n",
      "15. The Introduction  to Statistical  Learning  : David,    \n",
      "16. The Introduction  to Statistical  Learning  : David,    \n",
      "17. The Introduction  to Statistical  Learning  : David,    \n",
      "18. The Introduction  to Statistical  Learning  : David,    \n",
      "19. The Introduction  to Statistical  Learning  : David,    \n",
      "20. The Introduction  to Statistical   : David,    \n",
      "21. The Introduction  to Statistical  Learning  : David,    \n",
      "22. The Introduction  to Statistical   : David,    \n",
      "23. The Introduction  to Statistical   : David,    \n",
      "24. The Introduction  to Statistical   : David,    \n",
      "25. The Introduction  to Statistical   : David,  \n",
      "26. The Introduction  to Statistical   : David,  \n",
      "27. The Introduction  to Statistical   : David,   \n",
      "28. The Introduction  to Statistical   : David,   \n",
      "29. The Introduction  to Statistical   : David,   \n",
      "30. The Introduction  to Statistical   : David\n"
     ]
    }
   ],
   "source": [
    "def query_pdf(query):\n",
    "    # Initialize embeddings using the updated HuggingFaceEmbeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "    # Define the path for the vector store\n",
    "    vector_store_path = \"vector_store/test\"\n",
    "\n",
    "    # Check if the vectors are already persisted\n",
    "    try:\n",
    "        faiss_index_path = f\"{vector_store_path}/index.faiss\"\n",
    "        faiss_metadata_path = f\"{vector_store_path}/index.pkl\"\n",
    "        if os.path.exists(faiss_index_path) and os.path.exists(faiss_metadata_path):\n",
    "            # Load persisted vector store\n",
    "            persisted_vectorstore = FAISS.load_local(\n",
    "                vector_store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            print(\"✅ Loaded vector store from local storage.\")\n",
    "        else:\n",
    "            raise FileNotFoundError\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠️ Vector store not found. Creating a new vector store...\")\n",
    "\n",
    "        # Load document using PyPDFLoader\n",
    "        loader = PyPDFLoader(\"./documents/SC1015_BasicInformation.pdf\")\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Split document into chunks\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=30,\n",
    "            separator=\"\\n\"\n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Create vectors using FAISS\n",
    "        vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "        # Persist the vectors locally on disk\n",
    "        vectorstore.save_local(vector_store_path)\n",
    "        print(\"💾 Vector store saved locally.\")\n",
    "\n",
    "        # Load the persisted vector store after saving\n",
    "        persisted_vectorstore = FAISS.load_local(\n",
    "            vector_store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    # Initialize the Hugging Face text-generation pipeline\n",
    "    text_gen_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"distilgpt2\",  # You can choose a more powerful model if needed\n",
    "        tokenizer=\"distilgpt2\",\n",
    "        framework=\"pt\",  # Use \"tf\" if you prefer TensorFlow\n",
    "        max_length=512,  # Adjust as needed\n",
    "        max_new_tokens=512,   # Adjust as needed\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Wrap the pipeline with LangChain's HuggingFacePipeline\n",
    "    llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n",
    "\n",
    "    # Initialize the RetrievalQA chain with the wrapped LLM\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=persisted_vectorstore.as_retriever(),\n",
    "        verbose=True  # Optional: Set to True for more detailed logs\n",
    "    )\n",
    "\n",
    "    # Run the query and return the result\n",
    "    result = qa.run(query)\n",
    "    print(\"\\n📝 Query Result:\")\n",
    "    print(result)\n",
    "\n",
    "\n",
    "query_pdf(\"What is the purpose of SC1015?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
