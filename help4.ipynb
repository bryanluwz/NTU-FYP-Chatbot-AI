{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values\n",
    "import torch\n",
    "\n",
    "from src.QA_Chain import QA_Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found lessgoo..., setting memory fraction to 80%\n",
      "Embedding model path: c:\\Users\\bryan\\Documents\\GitHub\\NTU-FYP-Chatbot-AI\\models\\embedding_model\n",
      "Vector store path: c:\\Users\\bryan\\Documents\\GitHub\\NTU-FYP-Chatbot-AI\\models\\vector_store\n",
      "LLM Model path: c:\\Users\\bryan\\Documents\\GitHub\\NTU-FYP-Chatbot-AI\\models\\llm_model\n",
      "Loading documents from directory: c:\\Users\\bryan\\Documents\\GitHub\\NTU-FYP-Chatbot-AI\\documents\\test\n"
     ]
    }
   ],
   "source": [
    "# Set torch to use the GPU memory at 80% capacity\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU found lessgoo..., setting memory fraction to 80%\")\n",
    "    torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# MODEL_NAME = 'meta-llama/Llama-3.2-1B'\n",
    "MODEL_NAME = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "# MODEL_NAME = 'distilgpt2'\n",
    "EMBEDDING_NAME = 'sentence-transformers/paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "EMBEDDING_MODEL_PATH = os.path.abspath(config.get(\n",
    "    'EMBEDDING_MODEL_PATH', './models/embedding_model'))\n",
    "\n",
    "VECTOR_STORE_PATH = os.path.abspath(config.get(\n",
    "    'VECTOR_STORE_PATH', './models/vector_store'))\n",
    "\n",
    "LLM_MODEL_PATH = os.path.abspath(\n",
    "    config.get('LLM_MODEL_PATH', './models/model'))\n",
    "\n",
    "print(\"Embedding model path:\", EMBEDDING_MODEL_PATH)\n",
    "print(\"Vector store path:\", VECTOR_STORE_PATH)\n",
    "print(\"LLM Model path:\", LLM_MODEL_PATH)\n",
    "\n",
    "DOCUMENT_PARENT_DIR_PATH = os.path.abspath(\n",
    "    config.get('DOCUMENT_DIR_PATH', './documents'))\n",
    "DOCUMENT_DIR_NAME = 'test'\n",
    "DOCUMENT_DIR_PATH = os.path.join(DOCUMENT_PARENT_DIR_PATH, DOCUMENT_DIR_NAME)\n",
    "\n",
    "print(\"Loading documents from directory:\", DOCUMENT_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded vector store from local storage.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    qa_chain.destroy()\n",
    "    print(\"Someone is already running the QA Chain, destroying the old instance.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "qa_chain = QA_Chain()\n",
    "\n",
    "qa_chain.load_embeddings_model(\n",
    "    EMBEDDING_NAME, embedding_model_path=EMBEDDING_MODEL_PATH)\n",
    "\n",
    "vector_store_path = os.path.join(\n",
    "    VECTOR_STORE_PATH, f\"{DOCUMENT_DIR_NAME}_{qa_chain.embeddings.model_name}\")\n",
    "file_paths = [os.path.join(DOCUMENT_DIR_PATH, filepath)\n",
    "              for filepath in os.listdir(DOCUMENT_DIR_PATH)]\n",
    "file_paths_abs = [os.path.abspath(file_path) for file_path in file_paths]\n",
    "\n",
    "qa_chain.load_vector_store(vector_store_path, file_paths_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading model from c:\\Users\\bryan\\Documents\\GitHub\\NTU-FYP-Chatbot-AI\\models\\llm_model\\meta-llama/Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: MAKE SURE YOU'RE AUTHENTICATED AND HAVE ACCESS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "custom_prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Persona:\n",
    "You are an AI model that provides short, concise answers.\n",
    "If you do not know the answer, respond with \"I don't know.\"\n",
    "Do not make up information. \n",
    "\n",
    "Only generate one answer, do not generate questions.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}?\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# custom_prompt_template = None\n",
    "\n",
    "qa_chain.initialize_llm(model_name=MODEL_NAME,\n",
    "                        max_new_tokens=512, model_path=LLM_MODEL_PATH, temperature=0.5)\n",
    "\n",
    "# For top_k some reason >= 4, then the thing cannot work\n",
    "qa_chain.initialize_qa_chain(prompt_template=custom_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e3f1d82b2647ffbfbbf5d90317a22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "query = \"is this a pass fail course?\"\n",
    "\n",
    "result = qa_chain.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is this a pass fail course?\n",
      "===== Answer =====\n",
      "No, it is not a Pass/Fail course. You will be graded based on the components mentioned below. It is mandatory that you \"attempt\" at least 80% of the graded components to Pass this course. This means you can't simply drop the Mini-Project (30%) or the AI Theory Quiz (25%). You may miss a few components, if you have to, but the total weight of the components you miss should not be more than 20%. Be careful, and choose wisely, in case you do need to miss out on any component.\n"
     ]
    }
   ],
   "source": [
    "print(query)\n",
    "print(\"===== Answer =====\")\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
